{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from mcq import mcq_utils\n",
    "\n",
    "dataset = load_dataset('UCSD-GENIE/ClimaQA', data_files=\"climaqa_gold/mcq/mcq_benchmark.csv\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "\n",
    "mcq_df = dataset.to_pandas()\n",
    "mcq_df = mcq_df[mcq_df['Validation'] == True]\n",
    "mcq_list = mcq_utils.pandas_to_mcq(mcq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcq.mcq_agent import MCQAgent\n",
    "from mcq.mcq_fewshot_agent import MCQFewShotAgent\n",
    "from mcq.mcq_rag_agent import MCQRagAgent\n",
    "from llm.openai_llm import OpenAIILLM\n",
    "from llm.together_ai_llm import TogetherAILLM\n",
    "\n",
    "# Default LLM Agents\n",
    "mcq_agents = [\n",
    "    MCQAgent(OpenAIILLM('gpt-3.5-turbo')),\n",
    "    MCQAgent(OpenAIILLM('gpt-4o')),\n",
    "    MCQAgent(TogetherAILLM('llama3-70b')),\n",
    "    MCQAgent(TogetherAILLM('mixtral-8x22b')),\n",
    "    MCQAgent(TogetherAILLM('gemma-27b'))\n",
    "]\n",
    "\n",
    "\n",
    "# LLM Agents that answer with Few-shot prompting\n",
    "# mcq_agents = [\n",
    "#     MCQFewShotAgent(OpenAIILLM('gpt-3.5-turbo')),\n",
    "#     MCQFewShotAgent(OpenAIILLM('gpt-4o')),\n",
    "#     MCQFewShotAgent(TogetherAILLM('llama3-70b')),\n",
    "#     MCQFewShotAgent(TogetherAILLM('mixtral-8x22b')),\n",
    "#     MCQFewShotAgent(TogetherAILLM('gemma-27b'))\n",
    "# ]\n",
    "\n",
    "# LLM Agents that answer with RAG prompting. Need to create a chroma vectordb for these to work\n",
    "# mcq_agents = [\n",
    "#     MCQRagAgent(OpenAIILLM('gpt-3.5-turbo')),\n",
    "#     MCQRagAgent(OpenAIILLM('gpt-4o')),\n",
    "#     MCQRagAgent(TogetherAILLM('llama3-70b')),\n",
    "#     MCQRagAgent(TogetherAILLM('mixtral-8x22b')),\n",
    "#     MCQRagAgent(TogetherAILLM('gemma-27b'))\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the knowledge of the horizontal distribution of the absolute momentum M allow us to characterize the stability of the atmosphere?\n",
      "\n",
      "a) By evaluating the static stability\n",
      "b) By analyzing the potential instability\n",
      "c) By determining the convective instability\n",
      "d) By assessing the inertial stability\n",
      "\n",
      "Correct Answer: d\n",
      "\n",
      "gpt-3.5-turbo: a\n",
      "gpt-4o: d\n",
      "llama3-70b: d\n",
      "mixtral-8x22b: a\n",
      "gemma-27b: d\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "mcq = random.choice(mcq_list)\n",
    "mcq_utils.get_results_for_question(mcq_agents, mcq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "result_dir = 'results/mcq'\n",
    "\n",
    "num_trials = 3\n",
    "for i in range(num_trials):\n",
    "    print(f'\\nRunning trial {i+1}\\n')\n",
    "    trial_dir = os.path.join(result_dir, f'trial_{i+1}')\n",
    "    for agent in mcq_agents:\n",
    "        print(f'Answering with {agent.name}')\n",
    "        mcq_utils.generate_and_save_answer(agent, mcq_df, trial_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcq import mcq_utils\n",
    "\n",
    "mcq_utils.evaluate_result(result_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
